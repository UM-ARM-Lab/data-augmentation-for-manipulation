{
  full_retrain_classifier: false
  trials_per_iteration: 1
  trials_generator_type: random
  fine_tune_classifier: {
    fine_tune_conv: true
    fine_tune_dense: true
    fine_tune_lstm: true
    fine_tune_output: true
    learning_rate: 0.001
    epochs: 25
    early_stopping: false
  }
  fine_tune_recovery: null
  planner_params_update: {
    termination_criteria:
    {
    }
  }
  results_to_classifier_dataset: {
    val_split: 0.0
    test_split: 0.0
  }
  results_to_recovery_dataset: null
  labeling_params_update: {
    augmentation: {
      version: 6.2
      # weights for the different terms in the objective
      sdf_grad_weight: 1.0
      bbox_weight: 0.05
      delta_min_dist_weight: 0.1
      invariance_weight: 1.0
      invariance_threshold: 0.1  # loss than this much model error is tolerable and incurs no cost

      # hyperparameters of the optimization loop
      lr_decay: 0.95
      lr_decay_steps: 1
      max_steps: 25
      n_outer_iters: 5
      target_trans_lim: 0.25
      target_euler_lim: 0.5
      not_progressing_threshold: 0.001
      step_size_threshold: 0.0003
      step_size: 0.3
      grad_clip: 0.1

      # for the final check of the constraints
      max_env_violations: 8
      delta_min_dist_threshold: 0.055

      # IK
      rand_dist: 0.1
      max_collision_check_attempts: 100

      # misc
      num_object_interp: 2
      good_enough_percentile: 1.0  # 1 accept all target transforms, 0.1 means use the top 10%
      invariance_model: /media/shared/invariance_trials/relu/July_16_11-30-35_47c46e451e/best_checkpoint
    }
  }
}
