{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpflow as gpf\n",
    "import numpy as np\n",
    "import gpflow.multioutput.kernels as mk\n",
    "import gpflow.multioutput.features as mf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "# from latent_constraint_learning import experiments_util\n",
    "tf.logging.set_verbosity(tf.logging.FATAL)\n",
    "plt.style.use(\"slides\")\n",
    "np.set_printoptions(suppress=True, precision=4, linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_forward_data(data, traj_idx_start=0, traj_idx_end=-1, take_every=1):\n",
    "    states = data['states'][traj_idx_start:traj_idx_end]\n",
    "    for traj_idx  in range(states.shape[0]):\n",
    "        states[traj_idx, :, 0] -= states[traj_idx, 0, 4]\n",
    "        states[traj_idx, :, 1] -= states[traj_idx, 0, 5]\n",
    "        states[traj_idx, :, 2] -= states[traj_idx, 0, 4]\n",
    "        states[traj_idx, :, 3] -= states[traj_idx, 0, 5]\n",
    "        states[traj_idx, :, 4] -= states[traj_idx, 0, 4]\n",
    "        states[traj_idx, :, 5] -= states[traj_idx, 0, 5]\n",
    "    actions = data['actions'][traj_idx_start:traj_idx_end]\n",
    "    state_dim = states.shape[2]\n",
    "    action_dim = actions.shape[2]\n",
    "    s = states[:, :-take_every, :]\n",
    "    s_next = states[:, take_every:, :]\n",
    "    s_flat = s.reshape(-1, state_dim)\n",
    "    s_next_flat = s_next.reshape(-1, state_dim)\n",
    "    if take_every == 1:\n",
    "        u_flat = actions[:, :, :].reshape(-1, action_dim)\n",
    "    else:\n",
    "        u_flat = actions[:, :-(take_every-1), :].reshape(-1, action_dim)\n",
    "    \n",
    "    combined_x = np.concatenate((s_flat, u_flat), axis=1)\n",
    "    # make data more zero centered by making things relative to the head\n",
    "    # input is [x_tail - x_head, y_tail - y_head, x_mid - x_head, y_mid - y_head, 0, 0, vx_head, vy_head]\n",
    "    # output is [delta x_tail, delta y_tail, delt x_mid, delta y_mid, delta x_head, delta_y_head]\n",
    "    x_flat = s_flat\n",
    "    # predict the delta only\n",
    "    y_flat = s_next_flat - s_flat\n",
    "    # when doing rollouts, first you have to take the \n",
    "    x_trajs = s\n",
    "    u_trajs = actions\n",
    "    return x_flat, y_flat, u_flat, combined_x, x_trajs, u_trajs\n",
    "\n",
    "def format_inverse_data(data, traj_idx_start=0, traj_idx_end=-1, take_every=1):\n",
    "    states = data['states'][traj_idx_start:traj_idx_end]\n",
    "    for traj_idx  in range(states.shape[0]):\n",
    "        states[traj_idx, :, 0] -= states[traj_idx, 0, 4]\n",
    "        states[traj_idx, :, 1] -= states[traj_idx, 0, 5]\n",
    "        states[traj_idx, :, 2] -= states[traj_idx, 0, 4]\n",
    "        states[traj_idx, :, 3] -= states[traj_idx, 0, 5]\n",
    "        states[traj_idx, :, 4] -= states[traj_idx, 0, 4]\n",
    "        states[traj_idx, :, 5] -= states[traj_idx, 0, 5]\n",
    "    actions = data['actions'][traj_idx_start:traj_idx_end]\n",
    "    state_dim = states.shape[2]\n",
    "    action_dim = actions.shape[2]\n",
    "    s = states[:, :-take_every, :]\n",
    "    s_next = states[:, take_every:, :]\n",
    "    s_flat = s.reshape(-1, state_dim)\n",
    "    s_next_flat = s_next.reshape(-1, state_dim)\n",
    "    if take_every == 1:\n",
    "        u_flat = actions[:, :, :].reshape(-1, action_dim)\n",
    "    else:\n",
    "        u_flat = actions[:, :-(take_every-1), :].reshape(-1, action_dim)\n",
    "    \n",
    "    combined_x = np.concatenate((s_flat, s_next_flat - s_flat), axis=1)\n",
    "    return combined_x, u_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trajectories: 240\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"240_50_random4.npz\")\n",
    "print('number of trajectories: {}'.format(data['states'].shape[0]))\n",
    "train_idx_start = 0\n",
    "train_idx_end = 200\n",
    "test_idx_start = 200\n",
    "test_idx_end = 240 \n",
    "take_every = 1\n",
    "\n",
    "train_x_flat, train_y_flat, train_u_flat, combined_train_x, train_x_trajs, train_u_trajs = format_forward_data(data, train_idx_start, train_idx_end, take_every)\n",
    "test_x_flat, test_y_flat, test_u_flat, combined_test_x, test_x_trajs, test_u_trajs = format_forward_data(data, test_idx_start, test_idx_end, take_every)\n",
    "train_inverse_combined_x, train_inverse_y = format_inverse_data(data, train_idx_start, train_idx_end, take_every)\n",
    "test_inverse_combined_x, test_inverse_y = format_inverse_data(data, test_idx_start, test_idx_end, take_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel:\n",
    "    \n",
    "    def __init__(self, X, Y, M=100):\n",
    "        self.N, self.D = X.shape\n",
    "        _, self.P = Y.shape\n",
    "        self.M = M # number of inducing points\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "        self.model_def = {\n",
    "            'class': gpf.kernels.SquaredExponential,\n",
    "            'initial_hyper_params': {\n",
    "                'lengthscales': 1.0\n",
    "            },\n",
    "            'initial_likelihood_variance': [0.1]*self.P\n",
    "        }\n",
    "        self.kern_list = [self.model_def['class'](self.D, **self.model_def['initial_hyper_params']) for _ in range(self.P)]\n",
    "        self.kernel = mk.SeparateIndependentMok(self.kern_list)\n",
    "\n",
    "        self.Zs = [X[np.random.permutation(self.N)[:self.M],...].copy() for _ in range(self.P)]\n",
    "        # initialise as list inducing features\n",
    "        self.feature_list = [gpf.features.InducingPoints(Z) for Z in self.Zs]\n",
    "        # create multioutput features from feature_list\n",
    "        self.feature = mf.SeparateIndependentMof(self.feature_list)\n",
    "\n",
    "        self.likelihood_variance = self.model_def['initial_likelihood_variance']\n",
    "        self.likelihood = gpf.likelihoods.Gaussian(self.likelihood_variance)\n",
    "        self.model = gpf.models.SVGP(self.X, self.Y, self.kernel, self.likelihood, feat=self.feature)\n",
    "        self.opt = gpf.train.ScipyOptimizer()\n",
    "\n",
    "    def train(self, print_time=True, display=True, maxiter=300):\n",
    "        t0 = time()\n",
    "        self.opt.minimize(self.model, disp=display, maxiter=maxiter)\n",
    "        dt = time() - t0\n",
    "        if print_time:\n",
    "            print(\"training time: {}s\".format(dt))\n",
    "            \n",
    "    def metadata(self):\n",
    "        return {\n",
    "            'N': self.N,\n",
    "            'D': self.D,\n",
    "            'P': self.P,\n",
    "            'M': self.M,\n",
    "            'kernel_type': self.model_def['class'].__name__,\n",
    "            'initial_hyper_params': self.model_def['initial_hyper_params'],\n",
    "            'initial_likelihood_variance': self.model_def['initial_likelihood_variance'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpf.reset_default_graph_and_session()\n",
    "fwd_model = MyModel(combined_train_x, train_y_flat)\n",
    "inv_model = MyModel(train_inverse_combined_x, train_inverse_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 30.246812343597412s\n"
     ]
    }
   ],
   "source": [
    "maxiter = 100\n",
    "fwd_model.train(maxiter=maxiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = gpf.saver.Saver()\n",
    "metadata = {\n",
    "    'maxiter': maxiter,\n",
    "}\n",
    "# metadata.update(fwd_model.metadata())\n",
    "# log_path = experiments_util.experiment_name('separate_independant', 'gpf')\n",
    "# full_log_path = os.path.join(os.getcwd(), 'log_data', log_path)\n",
    "# experiments_util.make_log_dir(full_log_path)\n",
    "# experiments_util.write_metadata(metadata, log_path)\n",
    "# model_path = os.path.join(full_log_path, 'forward_model')\n",
    "# saver.save(model_path, fwd_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 10.653449535369873s\n"
     ]
    }
   ],
   "source": [
    "# pretty sure we don't need to learn this inverse model, we can use non-linear optimization to solve this with the forward model?\n",
    "inv_model.train(maxiter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_total_node_position_error(my_model, test_x, test_y):\n",
    "    \"\"\" compute the euclidian distance for each node in pred_y[i] to each node in test_y[i], averaged over all i using the max likelihood prediction\"\"\"\n",
    "    pred_delta_x_mean, pred_delta_x_sigma = my_model.model.predict_y(test_x)\n",
    "    tail_error = np.linalg.norm(pred_delta_x_mean[:, 0:2] - test_y[:, 0:2], axis=1)\n",
    "    mid_error = np.linalg.norm(pred_delta_x_mean[:, 2:4] - test_y[:, 2:4], axis=1)\n",
    "    head_error = np.linalg.norm(pred_delta_x_mean[:, 4:6] - test_y[:, 4:6], axis=1)\n",
    "    print('tail position error (min/max/mean)', tail_error.min(), tail_error.max(), tail_error.mean())\n",
    "    print('mid position error (min/max/mean)', mid_error.min(), mid_error.max(), mid_error.mean())\n",
    "    print('head position error (min/max/mean)', head_error.min(), head_error.max(), head_error.mean())\n",
    "    total_node_error = tail_error + mid_error + head_error\n",
    "    print('min total error', total_node_error.min())\n",
    "    print('max total error', total_node_error.max())\n",
    "    print('mean total error', total_node_error.mean())\n",
    "\n",
    "def mean_control_error(my_model, test_x, test_y):\n",
    "    \"\"\" compute the euclidian distance between the predicted control and the true control\"\"\"\n",
    "    pred_u_mean, pred_u_sigma = my_model.model.predict_y(test_x)\n",
    "    pred_speeds = np.linalg.norm(pred_u_mean, axis=1)\n",
    "    \n",
    "    speed_error = np.linalg.norm(pred_u_mean - test_y, axis=1)\n",
    "    \n",
    "    pred_u_norm = np.linalg.norm(pred_u_mean, axis=1)\n",
    "    test_y_norm = np.linalg.norm(test_y, axis=1)\n",
    "    \n",
    "    # compute dot product of each column of a with each column of b\n",
    "    angle_error = np.arccos(np.einsum('ij,ij->i', pred_u_mean, test_y) / (pred_u_norm*test_y_norm))\n",
    "\n",
    "    print('min pred speed', np.min(pred_speeds))\n",
    "    print('max pred speed', np.max(pred_speeds))\n",
    "    print('mean pred speed', np.mean(pred_speeds))\n",
    "    \n",
    "    print(\"min speed error = {:0.3f} m/s\".format(speed_error.min()))\n",
    "    print(\"max speed error = {:0.3f} m/s\".format(speed_error.max()))\n",
    "    print(\"mean speed error = {:0.3f} m/s\".format(speed_error.mean()))\n",
    "    \n",
    "    print(\"min angle error = {:0.3f} m/s\".format(angle_error.min()))\n",
    "    print(\"max angle error = {:0.3f} m/s\".format(angle_error.max()))\n",
    "    print(\"mean angle error = {:0.3f} m/s\".format(angle_error.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min displacement in test 0.0010301430513861287\n",
      "max displacement in test 0.30061229261431177\n",
      "mean displacement in test 0.11060115018261646\n"
     ]
    }
   ],
   "source": [
    "total_displacement = np.linalg.norm(test_y_flat[:, 0:2], axis=1) + np.linalg.norm(test_y_flat[:, 2:4], axis=1) + np.linalg.norm(test_y_flat[:, 4:6], axis=1)\n",
    "print('min displacement in test', np.min(total_displacement))\n",
    "print('max displacement in test', np.max(total_displacement))\n",
    "print('mean displacement in test', np.mean(total_displacement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tail position error (min/max/mean) 0.0002787365327200829 0.09368788255045733 0.024556553951345116\n",
      "mid position error (min/max/mean) 0.0008177865214450011 0.08580572216234945 0.021676008794372283\n",
      "head position error (min/max/mean) 0.00026787803611576553 0.057631217648513165 0.00663875795703223\n",
      "min total error 0.007760264031475282\n",
      "max total error 0.17160683798073945\n",
      "mean total error 0.05287132070274963\n"
     ]
    }
   ],
   "source": [
    "# error for trained model\n",
    "mean_total_node_position_error(fwd_model, combined_test_x, test_y_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min/max/mean/median of control input:\n",
      "=====training=====\n",
      "0.0027032138935026984\n",
      "0.9903389473967043\n",
      "0.47104272124346014\n",
      "0.43892548204609216\n",
      "=====testing=====\n",
      "0.0782460996904688\n",
      "0.9903685221395796\n",
      "0.5768620286866841\n",
      "0.6301570903618964\n",
      "=====prediction=====\n",
      "min pred speed 0.0027983151681439038\n",
      "max pred speed 1.0509858146649491\n",
      "mean pred speed 0.5070520906323781\n",
      "min speed error = 0.002 m/s\n",
      "max speed error = 0.717 m/s\n",
      "mean speed error = 0.149 m/s\n",
      "min angle error = 0.000 m/s\n",
      "max angle error = 2.526 m/s\n",
      "mean angle error = 0.166 m/s\n"
     ]
    }
   ],
   "source": [
    "# check that the statistics of the control predictions match the training data\n",
    "print(\"min/max/mean/median of control input:\")\n",
    "train_speeds = np.linalg.norm(train_inverse_y, axis=1)\n",
    "print(\"=====training=====\")\n",
    "print(np.min(train_speeds))\n",
    "print(np.max(train_speeds))\n",
    "print(np.mean(train_speeds))\n",
    "print(np.median(train_speeds))\n",
    "test_speeds = np.linalg.norm(test_inverse_y, axis=1)\n",
    "print(\"=====testing=====\")\n",
    "print(np.min(test_speeds))\n",
    "print(np.max(test_speeds))\n",
    "print(np.mean(test_speeds))\n",
    "print(np.median(test_speeds))\n",
    "print(\"=====prediction=====\")\n",
    "mean_control_error(inv_model, test_inverse_combined_x, test_inverse_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(fwd_model, traj, steps=1, initial_variance = 0.00001):\n",
    "    test_x_traj, test_u_traj  = traj\n",
    "    traj_length = test_x_traj.shape[0]\n",
    "    \n",
    "    assert steps < traj_length, \"steps {} > traj length {}!\".format(steps, traj_length)\n",
    "    \n",
    "    if test_x_traj.shape[0] > test_u_traj.shape[0]:\n",
    "        test_x_traj = test_x_traj[:test_u_traj.shape[0]]\n",
    "        \n",
    "    mu_combined_test_x = np.hstack((test_x_traj, test_u_traj))\n",
    "        \n",
    "    # define the initial distribution\n",
    "    mu_combined_test_x_t = mu_combined_test_x[0]\n",
    "    sigma_combined_test_x = np.eye(fwd_model.D) * initial_variance\n",
    "    # assume no control variance\n",
    "    sigma_combined_test_x[-2, -2] = 0\n",
    "    sigma_combined_test_x[-1, -1] = 0\n",
    "    \n",
    "    # sample from that initial distribution to get initial particles to feed into the GP\n",
    "    num_particles = 50\n",
    "    combined_x_t_particles = np.random.multivariate_normal(mu_combined_test_x_t, sigma_combined_test_x, num_particles)\n",
    "    particles = np.zeros((steps, num_particles, fwd_model.D))\n",
    "    \n",
    "    for t in range(steps):\n",
    "        particles[t] = combined_x_t_particles\n",
    "        mu_delta_x_t_plus_1s, var_delta_x_t_plus_1s = fwd_model.model.predict_y(combined_x_t_particles)\n",
    "        # sample point from the gaussian prediction\n",
    "        combined_x_t_plus_1_particles = np.empty_like(combined_x_t_particles)\n",
    "        for j, (mu_delta_x_t_plus_1_j, var_delta_x_t_plus_1_j) in enumerate(zip(mu_delta_x_t_plus_1s, var_delta_x_t_plus_1s)):\n",
    "            # We assumed that the GPs are independant for each output dimension, so the full covariance matrix is diagonal\n",
    "            sigma_delta_x_t_plus_1_j = np.diag(var_delta_x_t_plus_1_j)\n",
    "            u_t_plus_1_j = test_u_traj[t + 1]\n",
    "            delta_delta_x_t_j = np.random.multivariate_normal(mu_delta_x_t_plus_1_j, sigma_delta_x_t_plus_1_j)\n",
    "            # predict only gives the delta position, so we have to integrate here\n",
    "            delta_delta_combined_x_t_j = np.hstack((delta_delta_x_t_j, [0,0 ]))\n",
    "            combined_x_t_plus_1_particles[j] =  combined_x_t_particles[j] + delta_delta_combined_x_t_j\n",
    "        \n",
    "        combined_x_t_particles = combined_x_t_plus_1_particles\n",
    "            \n",
    "    return particles, test_x_traj, test_u_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_no_ground_truth(my_model, initial_x, u, steps=1, initial_variance = 0.00001):\n",
    "    # define the initial distribution\n",
    "    mu_combined_x_t =  np.hstack((initial_x, u))\n",
    "    sigma_combined_x_t = np.eye(my_model.D) * initial_variance\n",
    "    # assume no control variance\n",
    "    sigma_combined_x_t[-2, -2] = 0\n",
    "    sigma_combined_x_t[-1, -1] = 0\n",
    "    \n",
    "    # sample from that initial distribution to get initial particles to feed into the GP\n",
    "    num_particles = 5\n",
    "    combined_x_t_particles = np.random.multivariate_normal(mu_combined_x_t, sigma_combined_x_t, num_particles)\n",
    "    print(combined_x_t_particles)\n",
    "    particles = np.zeros((steps, num_particles, my_model.D))\n",
    "    \n",
    "    for t in range(steps):\n",
    "        particles[t] = combined_x_t_particles\n",
    "        mu_delta_x_t_plus_1s, var_delta_x_t_plus_1s = my_model.model.predict_y(combined_x_t_particles)\n",
    "        # sample point from the gaussian prediction\n",
    "        combined_x_t_plus_1_particles = np.empty_like(combined_x_t_particles)\n",
    "        for j, (mu_delta_x_t_plus_1_j, var_delta_x_t_plus_1_j) in enumerate(zip(mu_delta_x_t_plus_1s, var_delta_x_t_plus_1s)):\n",
    "            # We assumed that the GPs are independant for each output dimension, so the full covariance matrix is diagonal\n",
    "            sigma_delta_x_t_plus_1_j = np.diag(var_delta_x_t_plus_1_j)\n",
    "            delta_delta_x_t_j = np.random.multivariate_normal(mu_delta_x_t_plus_1_j, sigma_delta_x_t_plus_1_j)\n",
    "            # predict only gives the delta position, so we have to integrate here\n",
    "            delta_delta_combined_x_t_j = np.hstack((delta_delta_x_t_j, [0,0 ]))\n",
    "            combined_x_t_plus_1_particles[j] =  combined_x_t_particles[j] + delta_delta_combined_x_t_j\n",
    "        \n",
    "        combined_x_t_particles = combined_x_t_plus_1_particles\n",
    "            \n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "def plot_predict(particles, test_x_traj=None, test_u_traj=None):\n",
    "    T = particles.shape[0]\n",
    "    if test_x_traj is not None:\n",
    "        x0s = [test_x_traj[0, 0], test_x_traj[0, 2], test_x_traj[0, 4]]\n",
    "        y0s = [test_x_traj[0, 1], test_x_traj[0, 3], test_x_traj[0, 5]]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    \n",
    "    x_t_particles_data = []\n",
    "    for x_t_particle in particles[0]:\n",
    "        x_t_particles_xs = [x_t_particle[0], x_t_particle[2], x_t_particle[4]]\n",
    "        x_t_particles_ys = [x_t_particle[1], x_t_particle[3], x_t_particle[5]]\n",
    "        line = plt.plot(x_t_particles_xs, x_t_particles_ys, color='black', alpha=0.4)[0]\n",
    "        x_t_particles_data.append(line)\n",
    "            \n",
    "    \n",
    "    x_next_data = []\n",
    "    if test_x_traj is not None:\n",
    "        plt.plot(x0s, y0s, color='orange')\n",
    "        x_t_plus_1 = test_x_traj[1]\n",
    "        xs_next = [x_t_plus_1[0], x_t_plus_1[2], x_t_plus_1[4]]\n",
    "        ys_next = [x_t_plus_1[1], x_t_plus_1[3], x_t_plus_1[5]]\n",
    "        x_next_data = plt.plot(xs_next, ys_next, color='blue')[0]\n",
    "\n",
    "    if test_x_traj is not None:\n",
    "        plt.quiver(test_x_traj[0, 4], test_x_traj[0, 5], test_u_traj[0, 0], test_u_traj[0, 1], color='black')\n",
    "\n",
    "    plt.xlabel(\"x (m)\")\n",
    "    plt.ylabel(\"y (m)\")\n",
    "    plt.xlim([-8,8])\n",
    "    plt.ylim([-8,8])\n",
    "    custom_lines = [Line2D([0], [0], color='black', lw=1, alpha=0.1),\n",
    "                    Line2D([0], [0], color='orange', lw=1),\n",
    "                    Line2D([0], [0], color='blue', lw=1)]\n",
    "\n",
    "    l = plt.legend(custom_lines, ['predictions', r'$x_0$', r'$x_t$'])\n",
    "    \n",
    "    def update(t):\n",
    "        x_t_particles = particles[t]\n",
    "        for x_t_particle_d, x_t_particle in zip(x_t_particles_data, x_t_particles):\n",
    "            x_t_particles_xs = [x_t_particle[0], x_t_particle[2], x_t_particle[4]]\n",
    "            x_t_particles_ys = [x_t_particle[1], x_t_particle[3], x_t_particle[5]]\n",
    "            x_t_particle_d.set_xdata(x_t_particles_xs)\n",
    "            x_t_particle_d.set_ydata(x_t_particles_ys)\n",
    "        \n",
    "        if test_x_traj is not None:\n",
    "            x_t_plus_1 = test_x_traj[t]\n",
    "            xs_next = [x_t_plus_1[0], x_t_plus_1[2], x_t_plus_1[4]]\n",
    "            ys_next = [x_t_plus_1[1], x_t_plus_1[3], x_t_plus_1[5]]\n",
    "            x_next_data.set_xdata(xs_next)\n",
    "            x_next_data.set_ydata(ys_next)\n",
    "        ax = fig.gca()\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "\n",
    "    \n",
    "    anim = FuncAnimation(fig, update, frames=np.arange(0, T), interval=100)\n",
    "    plt.close()\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "# 9 and 13 are interesting\n",
    "# 14 is accurate\n",
    "trajectory_idx = 3\n",
    "traj = test_x_trajs[trajectory_idx], test_u_trajs[trajectory_idx]\n",
    "plot_args = predict(fwd_model, traj, steps=49)\n",
    "anim = plot_predict(*plot_args)\n",
    "anim_html = anim.to_jshtml()\n",
    "HTML(anim.to_jshtml())\n",
    "anim.save('test_ex_{}.gif'.format(trajectory_idx), writer='imagemagick', fps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-480b7d70f4fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minitial_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplot_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_no_ground_truth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0manim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_jshtml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "initial_x = np.array([0,1,0,0,0,-1])\n",
    "u = np.array([0, 0])\n",
    "plot_args = predict_no_ground_truth(m, initial_x, u, steps=200)\n",
    "anim = plot_predict(plot_args)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectory_following(fwd_model, inv_model, x_traj, steps=1, initial_variance = 0.00001):\n",
    "    traj_length = x_traj.shape[0]\n",
    "    \n",
    "    assert steps < traj_length, \"steps {} > traj length {}!\".format(steps, traj_length)\n",
    "    \n",
    "    # define the initial distribution\n",
    "    delta_x_1 = x_traj[1:2] - x_traj[0:1]\n",
    "    mu_u0, sigma_u0 = inv_model.model.predict_y(np.hstack((x_traj[0:1], delta_x_1)))\n",
    "    print(mu_u0.shape)\n",
    "    print(mu_u0, sigma_u0)\n",
    "    return\n",
    "    mu_combined_test_x_t = np.hstack((x_traj[0], u0))\n",
    "    sigma_combined_test_x = np.eye(fwd_model.D) * initial_variance\n",
    "    # assume no control variance\n",
    "    sigma_combined_test_x[-2, -2] = 0\n",
    "    sigma_combined_test_x[-1, -1] = 0\n",
    "    \n",
    "    # sample from that initial distribution to get initial particles to feed into the GP\n",
    "    num_particles = 2\n",
    "    combined_x_t_particles = np.random.multivariate_normal(mu_combined_test_x_t, sigma_combined_test_x, num_particles)\n",
    "    particles = np.zeros((steps, num_particles, fwd_model.D))\n",
    "    \n",
    "    for t in range(steps):\n",
    "        # take the next step in the reference trajectory and construct the input to the inverse model\n",
    "        traj_x_t_plus_1 = test_x_traj[t + 1]\n",
    "        delta_x_t_particles = combined_x_t_particles - traj_x_t_plus_1\n",
    "        inv_model_input = np.hstack((combined_x_t_particles, delta_x_t_particles))\n",
    "        u = inv_model.model.predict_y(inv_model_input)\n",
    "        print(u.shape)\n",
    "        \n",
    "        particles[t] = combined_x_t_particles\n",
    "        mu_delta_x_t_plus_1s, var_delta_x_t_plus_1s = fwd_model.model.predict_y(combined_x_t_particles)\n",
    "        # sample point from the gaussian prediction\n",
    "        combined_x_t_plus_1_particles = np.empty_like(combined_x_t_particles)\n",
    "        for j, (mu_delta_x_t_plus_1_j, var_delta_x_t_plus_1_j) in enumerate(zip(mu_delta_x_t_plus_1s, var_delta_x_t_plus_1s)):\n",
    "            # We assumed that the GPs are independant for each output dimension, so the full covariance matrix is diagonal\n",
    "            sigma_delta_x_t_plus_1_j = np.diag(var_delta_x_t_plus_1_j)\n",
    "            u_t_plus_1_j = test_u_traj[t + 1]\n",
    "            delta_delta_x_t_j = np.random.multivariate_normal(mu_delta_x_t_plus_1_j, sigma_delta_x_t_plus_1_j)\n",
    "            # predict only gives the delta position, so we have to integrate here\n",
    "            delta_delta_combined_x_t_j = np.hstack((delta_delta_x_t_j, [0,0 ]))\n",
    "            combined_x_t_plus_1_particles[j] =  combined_x_t_particles[j] + delta_delta_combined_x_t_j\n",
    "        \n",
    "        combined_x_t_particles = combined_x_t_plus_1_particles\n",
    "            \n",
    "    return particles, test_x_traj, test_u_traj\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "test_example_idx = 0\n",
    "plot_args = trajectory_following(fwd_model, inv_model, test_x_trajs[test_example_idx], steps=4)\n",
    "print(test_u_trajs[test_example_idx,0])\n",
    "# anim = plot_predict(*plot_args)\n",
    "# HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
